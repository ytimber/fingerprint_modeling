{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5530528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-32c4c0667fd3>:13: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(filepath)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import antropy as ant\n",
    "filepath = '/Users/yutingzhang/Downloads/subInfo_316_attention_210226.mat'\n",
    "arrays = {}\n",
    "f = h5py.File(filepath)\n",
    "for k, v in f.items():\n",
    "    arrays[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024625a",
   "metadata": {},
   "source": [
    "***THIS IS THE MULTIVARIATE METHOD***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f8ef040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the mat_rest and label into training and testing groups\n",
    "mat_rest = arrays['mat_rest']\n",
    "subList = arrays['subList']\n",
    "X = mat_rest\n",
    "Y = subList[1]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5005642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the data, fit on the training data and apply the fit to both training and testing datasets. \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35caf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate PCA model\n",
    "#either pick the variance to be used\n",
    "pca_var = PCA(0.2) # usually use 5 components to retain 20% variance\n",
    "#or pick the number of componenets to be used\n",
    "pca_com = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7dfea6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the data using pca_var\n",
    "pca_var.fit(X_train)\n",
    "#look for how many components are retained to achieve 95% variance\n",
    "pca_var.n_components_\n",
    "#apply the transform to both \n",
    "X_train = pca_var.transform(X_train)\n",
    "X_test = pca_var.transform(X_test)\n",
    "#35778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d6ff197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ran = round(len(mat_rest[0])/1000)\n",
    "n_sub = len(mat_rest)\n",
    "sub_result = []\n",
    "for i in range(n_sub):\n",
    "    sub_data = mat_rest[i]\n",
    "    for j in range(ran):\n",
    "        sphere_prior = j*1000\n",
    "        if j != ran-1:\n",
    "            sphere_posterior = (j+1)*1000\n",
    "        else:\n",
    "            sphere_posterior = len(mat_rest[0])-1\n",
    "        for k in range(ran):\n",
    "            seed_prior = k*1000\n",
    "            if k != ran-1:\n",
    "                seed_posterior = (k+1)*1000\n",
    "            else:\n",
    "                seed_posterior = len(mat_rest[0])-1\n",
    "            X = sub_data[sphere_prior:sphere_posterior]\n",
    "            Y = sub_data[seed_prior:seed_posterior]\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "            pca_com = PCA(n_components=3)\n",
    "            #fit the data using pca_var\n",
    "            pca_var.fit(X_train)\n",
    "            #look for how many components are retained to achieve 95% variance\n",
    "            pca_var.n_components_\n",
    "            #apply the transform to both \n",
    "            X_train = pca_var.transform(X_train)\n",
    "            X_test = pca_var.transform(X_test)\n",
    "            logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "            logisticRegr.fit(X_train,Y_train)\n",
    "            score = logisticRegr.score(X_test,Y_test)\n",
    "            sub_result.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1c96bf",
   "metadata": {},
   "source": [
    "****CPM METHOD STARTS FROM HERE!!!****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e694e9",
   "metadata": {},
   "source": [
    "**construct connectivity matrix and select the edge**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5cc104",
   "metadata": {},
   "source": [
    "*1. read in data, a matrix of 268*268 for each subject*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cba7182a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    filepath = path\n",
    "    arrays_conmat = {}\n",
    "    f = h5py.File(filepath)\n",
    "    \n",
    "    for k, v in f.items():\n",
    "        arrays_conmat[k] = np.array(v)\n",
    "        \n",
    "    return arrays_conmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411c2c3",
   "metadata": {},
   "source": [
    "*2. Extract the relevant data into numpy array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b6fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(arrays_conmat):\n",
    "    ##extract the phenotype data\n",
    "    pheno = arrays_conmat['subList'][1]\n",
    "    ##calculate the mean of the resting data\n",
    "    matrix = arrays_conmat['conMatRun268']\n",
    "    rest = matrix[0:4] #first four columns are resting state. \n",
    "    shape = np.shape(matrix)\n",
    "    new = []\n",
    "    \n",
    "    for sub in range(shape[1]):\n",
    "        sub_data = rest[:,sub,:,:]\n",
    "        sub_mean = np.mean(sub_data,axis=0) #Take the average between the four runs. \n",
    "        new.append(sub_mean) #the size of new is (316,268,268)\n",
    "        \n",
    "    return pheno, new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50689ff7",
   "metadata": {},
   "source": [
    "*Now, we have pheno, which is the pheno data (size:316, one for each subject), and new, which is the matrix (size: (316,268,268), one matrix for each of 316 subject)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da0bac5",
   "metadata": {},
   "source": [
    "*3. Calculate the correlation matrix and p-value matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_gen(pheno,new):\n",
    "    new_new = np.asarray(new)\n",
    "    cor_matrix = []\n",
    "    p_matrix = []\n",
    "    \n",
    "    for i in range(268):\n",
    "        for j in range(268):\n",
    "            data = new_new[:,i,j]\n",
    "            cor = stats.pearsonr(data,pheno)\n",
    "            cor_matrix.append(cor[0])\n",
    "            p_matrix.append(cor[1])\n",
    "            \n",
    "    cor_matrix = np.asarray(cor_matrix)\n",
    "    p_matrix = np.asarray(p_matrix)\n",
    "    \n",
    "    cor_matrix_resize = np.resize(cor_matrix,(268,268))\n",
    "    cor_matrix_resize = np.nan_to_num(cor_matrix_resize)\n",
    "    \n",
    "    p_matrix_resize = np.resize(p_matrix,(268,268))\n",
    "    p_matrix_resize = np.nan_to_num(p_matrix_resize)\n",
    "    \n",
    "    return cor_matrix_resize,p_matrix_resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cee44a",
   "metadata": {},
   "source": [
    "*4. With the correlation matrix and p-value matrix, we need to select positive and negative edges. If specific edge is greater than 0 and have p-value less than 0.05, it will be marked with 1, otherwise 0, in the pos_matrix. Likewise, if specific edge is less than 0 and have p-value less than 0.05, it will be marked with 1, otherwise 0, in the neg_matrix.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ddd17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_selection(cor_matrix_resize,p_matrix_resize,target_size):\n",
    "    \n",
    "    #create positive edge matrix and negative edge matrix\n",
    "    pos_matrix = []\n",
    "    neg_matrix = []\n",
    "    \n",
    "    assert np.shape(cor_matrix_resize) == target_size, \"Wrong size!\"\n",
    "    assert np.shape(p_matrix_resize) == target_size, \"Wrong size!\"\n",
    "    \n",
    "    for i in range(np.shape(cor_matrix_resize)[0]):\n",
    "        for j in range(np.shape(cor_matrix_resize)[0]):\n",
    "            cor = cor_matrix_resize[i,j]\n",
    "            p = p_matrix_resize[i,j]\n",
    "            if cor>0 and p<0.05:\n",
    "                pos_matrix.append(1)\n",
    "                neg_matrix.append(0)\n",
    "            elif cor<0 and p<0.05: \n",
    "                pos_matrix.append(0)\n",
    "                neg_matrix.append(1)\n",
    "            else:\n",
    "                pos_matrix.append(0)\n",
    "                neg_matrix.append(0)\n",
    "                \n",
    "    pos_matrix = np.reshape(pos_matrix,target_size)\n",
    "    neg_matrix = np.reshape(neg_matrix,target_size)\n",
    "    \n",
    "    return pos_matrix,neg_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a327e",
   "metadata": {},
   "source": [
    "*5. Now, we have two matrices, pos_mat and neg_mat, composed of 1 and 0. We will do an elementwise multiplication to select the positive edge and negative edge on the matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b20464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_multiply(all_sub_data,pos_matrix,neg_matrix):\n",
    "    all_pos = []\n",
    "    all_neg = []\n",
    "    for sub in all_sub_data:\n",
    "        sub_data_pos = np.multiply(sub,pos_matrix)\n",
    "        sub_data_neg = np.multiply(sub,neg_matrix)\n",
    "        sub_data_pos = np.reshape(sub_data_pos,(268,268))\n",
    "        sub_data_neg = np.reshape(sub_data_neg,(268,268))\n",
    "        all_pos.append(sub_data_pos)\n",
    "        all_neg.append(sub_data_neg)\n",
    "    return all_pos,all_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf106da",
   "metadata": {},
   "source": [
    "*6. Now we have the matrix of positive edge and matrix of negative edge. We want to calculate the summary statistics. Therefore, for each subject, there will be two numbers, with one corresponding to positive edges' summary stats and the other corresponding to negative edges' summary stats.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3cabcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_stats(all_pos,all_neg):\n",
    "    pos_sum = []\n",
    "    neg_sum = []\n",
    "    for pos_mat in all_pos:\n",
    "        pos = np.sum(pos_mat)\n",
    "        pos_sum.append(pos)\n",
    "    for neg_mat in all_neg:\n",
    "        neg = np.sum(neg_mat)\n",
    "        neg_sum.append(neg)\n",
    "    return pos_sum, neg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "33726f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    size = (268,268) #set up the size of each matrix for each subject\n",
    "    filepath = '/Users/yutingzhang/Downloads/conmat_HCP_run_316_201216.mat'\n",
    "    print('finished with filepath')\n",
    "    array = read_file(filepath) #read in the file\n",
    "    pheno, sub_mat = extract_data(array) #extract the matrix and phenotype data\n",
    "    print('finished with a')\n",
    "    cor_mat, p_mat = matrix_gen(pheno,sub_mat) #extract the correlation matrix and p-value matrix for all subjects\n",
    "    pos_mat, neg_mat = edge_selection(cor_mat,p_mat,size) #edge selection with two matrices, one for pos, one for neg\n",
    "    print('finished with b')\n",
    "    sub_pos, sub_neg = element_multiply(sub_mat,pos_mat,neg_mat) #using the matrix to apply to the actual subject data to select that\n",
    "    sub_sum_pos,sub_sum_neg = summary_stats(sub_pos,sub_neg) #calculate the summary statistics\n",
    "    print('finished with c')\n",
    "    return sub_sum_pos,sub_sum_neg,pheno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44b8c06",
   "metadata": {},
   "source": [
    "****Implementation****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c74ea",
   "metadata": {},
   "source": [
    "Note: Try leave-one-out ordinary least square regression, ridge regression, lasso regression, Elastic-Net, LARS-lasso, stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "85e2bd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with filepath\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-d8281cbeaf95>:4: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with a\n",
      "finished with b\n",
      "finished with c\n"
     ]
    }
   ],
   "source": [
    "sub_sum_pos,sub_sum_neg,pheno = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1347e5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316,)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sub_sum_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3e4f6",
   "metadata": {},
   "source": [
    "1. Ordinary least sqaure regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2c763c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def OLS(sub_sum_pos,sub_sum_neg,pheno):\n",
    "    original = []\n",
    "    predict = []\n",
    "    \n",
    "    for i in range(np.shape(sub_sum_pos)[0]):\n",
    "        print(\"On subject %s\" %i)\n",
    "        loo_sub_sum_pos = np.delete(sub_sum_pos,[i])\n",
    "        single_sub_sum_pos = sub_sum_pos[i]\n",
    "        loo_sub_sum_neg = np.delete(sub_sum_neg,[i])\n",
    "        single_sub_sum_neg = sub_sum_neg[i]\n",
    "        loo_pheno = np.delete(pheno,[i])\n",
    "        single_sub_pheno = pheno[i]\n",
    "        X_train = np.vstack((np.reshape(loo_sub_sum_pos,(1,315)), np.reshape(loo_sub_sum_neg,(1,315))))\n",
    "        X_train = np.transpose(X_train)\n",
    "        Y_train = loo_pheno\n",
    "        reg = LinearRegression().fit(X_train, Y_train)\n",
    "        prediction = reg.predict(np.transpose(np.vstack((single_sub_sum_pos,single_sub_sum_neg))))\n",
    "        original.append(single_sub_pheno)\n",
    "        predict.append(prediction[0])\n",
    "    return original,predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385f668",
   "metadata": {},
   "source": [
    "Evaluate the result using correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e313662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On subject 0\n",
      "On subject 1\n",
      "On subject 2\n",
      "On subject 3\n",
      "On subject 4\n",
      "On subject 5\n",
      "On subject 6\n",
      "On subject 7\n",
      "On subject 8\n",
      "On subject 9\n",
      "On subject 10\n",
      "On subject 11\n",
      "On subject 12\n",
      "On subject 13\n",
      "On subject 14\n",
      "On subject 15\n",
      "On subject 16\n",
      "On subject 17\n",
      "On subject 18\n",
      "On subject 19\n",
      "On subject 20\n",
      "On subject 21\n",
      "On subject 22\n",
      "On subject 23\n",
      "On subject 24\n",
      "On subject 25\n",
      "On subject 26\n",
      "On subject 27\n",
      "On subject 28\n",
      "On subject 29\n",
      "On subject 30\n",
      "On subject 31\n",
      "On subject 32\n",
      "On subject 33\n",
      "On subject 34\n",
      "On subject 35\n",
      "On subject 36\n",
      "On subject 37\n",
      "On subject 38\n",
      "On subject 39\n",
      "On subject 40\n",
      "On subject 41\n",
      "On subject 42\n",
      "On subject 43\n",
      "On subject 44\n",
      "On subject 45\n",
      "On subject 46\n",
      "On subject 47\n",
      "On subject 48\n",
      "On subject 49\n",
      "On subject 50\n",
      "On subject 51\n",
      "On subject 52\n",
      "On subject 53\n",
      "On subject 54\n",
      "On subject 55\n",
      "On subject 56\n",
      "On subject 57\n",
      "On subject 58\n",
      "On subject 59\n",
      "On subject 60\n",
      "On subject 61\n",
      "On subject 62\n",
      "On subject 63\n",
      "On subject 64\n",
      "On subject 65\n",
      "On subject 66\n",
      "On subject 67\n",
      "On subject 68\n",
      "On subject 69\n",
      "On subject 70\n",
      "On subject 71\n",
      "On subject 72\n",
      "On subject 73\n",
      "On subject 74\n",
      "On subject 75\n",
      "On subject 76\n",
      "On subject 77\n",
      "On subject 78\n",
      "On subject 79\n",
      "On subject 80\n",
      "On subject 81\n",
      "On subject 82\n",
      "On subject 83\n",
      "On subject 84\n",
      "On subject 85\n",
      "On subject 86\n",
      "On subject 87\n",
      "On subject 88\n",
      "On subject 89\n",
      "On subject 90\n",
      "On subject 91\n",
      "On subject 92\n",
      "On subject 93\n",
      "On subject 94\n",
      "On subject 95\n",
      "On subject 96\n",
      "On subject 97\n",
      "On subject 98\n",
      "On subject 99\n",
      "On subject 100\n",
      "On subject 101\n",
      "On subject 102\n",
      "On subject 103\n",
      "On subject 104\n",
      "On subject 105\n",
      "On subject 106\n",
      "On subject 107\n",
      "On subject 108\n",
      "On subject 109\n",
      "On subject 110\n",
      "On subject 111\n",
      "On subject 112\n",
      "On subject 113\n",
      "On subject 114\n",
      "On subject 115\n",
      "On subject 116\n",
      "On subject 117\n",
      "On subject 118\n",
      "On subject 119\n",
      "On subject 120\n",
      "On subject 121\n",
      "On subject 122\n",
      "On subject 123\n",
      "On subject 124\n",
      "On subject 125\n",
      "On subject 126\n",
      "On subject 127\n",
      "On subject 128\n",
      "On subject 129\n",
      "On subject 130\n",
      "On subject 131\n",
      "On subject 132\n",
      "On subject 133\n",
      "On subject 134\n",
      "On subject 135\n",
      "On subject 136\n",
      "On subject 137\n",
      "On subject 138\n",
      "On subject 139\n",
      "On subject 140\n",
      "On subject 141\n",
      "On subject 142\n",
      "On subject 143\n",
      "On subject 144\n",
      "On subject 145\n",
      "On subject 146\n",
      "On subject 147\n",
      "On subject 148\n",
      "On subject 149\n",
      "On subject 150\n",
      "On subject 151\n",
      "On subject 152\n",
      "On subject 153\n",
      "On subject 154\n",
      "On subject 155\n",
      "On subject 156\n",
      "On subject 157\n",
      "On subject 158\n",
      "On subject 159\n",
      "On subject 160\n",
      "On subject 161\n",
      "On subject 162\n",
      "On subject 163\n",
      "On subject 164\n",
      "On subject 165\n",
      "On subject 166\n",
      "On subject 167\n",
      "On subject 168\n",
      "On subject 169\n",
      "On subject 170\n",
      "On subject 171\n",
      "On subject 172\n",
      "On subject 173\n",
      "On subject 174\n",
      "On subject 175\n",
      "On subject 176\n",
      "On subject 177\n",
      "On subject 178\n",
      "On subject 179\n",
      "On subject 180\n",
      "On subject 181\n",
      "On subject 182\n",
      "On subject 183\n",
      "On subject 184\n",
      "On subject 185\n",
      "On subject 186\n",
      "On subject 187\n",
      "On subject 188\n",
      "On subject 189\n",
      "On subject 190\n",
      "On subject 191\n",
      "On subject 192\n",
      "On subject 193\n",
      "On subject 194\n",
      "On subject 195\n",
      "On subject 196\n",
      "On subject 197\n",
      "On subject 198\n",
      "On subject 199\n",
      "On subject 200\n",
      "On subject 201\n",
      "On subject 202\n",
      "On subject 203\n",
      "On subject 204\n",
      "On subject 205\n",
      "On subject 206\n",
      "On subject 207\n",
      "On subject 208\n",
      "On subject 209\n",
      "On subject 210\n",
      "On subject 211\n",
      "On subject 212\n",
      "On subject 213\n",
      "On subject 214\n",
      "On subject 215\n",
      "On subject 216\n",
      "On subject 217\n",
      "On subject 218\n",
      "On subject 219\n",
      "On subject 220\n",
      "On subject 221\n",
      "On subject 222\n",
      "On subject 223\n",
      "On subject 224\n",
      "On subject 225\n",
      "On subject 226\n",
      "On subject 227\n",
      "On subject 228\n",
      "On subject 229\n",
      "On subject 230\n",
      "On subject 231\n",
      "On subject 232\n",
      "On subject 233\n",
      "On subject 234\n",
      "On subject 235\n",
      "On subject 236\n",
      "On subject 237\n",
      "On subject 238\n",
      "On subject 239\n",
      "On subject 240\n",
      "On subject 241\n",
      "On subject 242\n",
      "On subject 243\n",
      "On subject 244\n",
      "On subject 245\n",
      "On subject 246\n",
      "On subject 247\n",
      "On subject 248\n",
      "On subject 249\n",
      "On subject 250\n",
      "On subject 251\n",
      "On subject 252\n",
      "On subject 253\n",
      "On subject 254\n",
      "On subject 255\n",
      "On subject 256\n",
      "On subject 257\n",
      "On subject 258\n",
      "On subject 259\n",
      "On subject 260\n",
      "On subject 261\n",
      "On subject 262\n",
      "On subject 263\n",
      "On subject 264\n",
      "On subject 265\n",
      "On subject 266\n",
      "On subject 267\n",
      "On subject 268\n",
      "On subject 269\n",
      "On subject 270\n",
      "On subject 271\n",
      "On subject 272\n",
      "On subject 273\n",
      "On subject 274\n",
      "On subject 275\n",
      "On subject 276\n",
      "On subject 277\n",
      "On subject 278\n",
      "On subject 279\n",
      "On subject 280\n",
      "On subject 281\n",
      "On subject 282\n",
      "On subject 283\n",
      "On subject 284\n",
      "On subject 285\n",
      "On subject 286\n",
      "On subject 287\n",
      "On subject 288\n",
      "On subject 289\n",
      "On subject 290\n",
      "On subject 291\n",
      "On subject 292\n",
      "On subject 293\n",
      "On subject 294\n",
      "On subject 295\n",
      "On subject 296\n",
      "On subject 297\n",
      "On subject 298\n",
      "On subject 299\n",
      "On subject 300\n",
      "On subject 301\n",
      "On subject 302\n",
      "On subject 303\n",
      "On subject 304\n",
      "On subject 305\n",
      "On subject 306\n",
      "On subject 307\n",
      "On subject 308\n",
      "On subject 309\n",
      "On subject 310\n",
      "On subject 311\n",
      "On subject 312\n",
      "On subject 313\n",
      "On subject 314\n",
      "On subject 315\n",
      "Correlation of the predicted and original score is 0.5894942746983071.\n"
     ]
    }
   ],
   "source": [
    "original, predict = OLS(sub_sum_pos,sub_sum_neg,pheno)\n",
    "cor = np.corrcoef(np.asarray(original),np.asarray(predict))[0][1]\n",
    "print(\"Correlation of the predicted and original score is %s.\" %cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89ae37",
   "metadata": {},
   "source": [
    "2. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d324567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "def ridgeRegression(sub_sum_pos,sub_sum_neg,pheno):\n",
    "    original = []\n",
    "    predict = []\n",
    "\n",
    "    for i in range(np.shape(sub_sum_pos)[0]):\n",
    "        print(\"On subject %s\" %i)\n",
    "        loo_sub_sum_pos = np.delete(sub_sum_pos,[i])\n",
    "        single_sub_sum_pos = sub_sum_pos[i]\n",
    "        loo_sub_sum_neg = np.delete(sub_sum_neg,[i])\n",
    "        single_sub_sum_neg = sub_sum_neg[i]\n",
    "        loo_pheno = np.delete(pheno,[i])\n",
    "        single_sub_pheno = pheno[i]\n",
    "        X_train = np.vstack((np.reshape(loo_sub_sum_pos,(1,315)), np.reshape(loo_sub_sum_neg,(1,315))))\n",
    "        X_train = np.transpose(X_train)\n",
    "        Y_train = loo_pheno\n",
    "        reg = Ridge(alpha = 0.0).fit(X_train, Y_train)\n",
    "        prediction = reg.predict(np.transpose(np.vstack((single_sub_sum_pos,single_sub_sum_neg))))\n",
    "        original.append(single_sub_pheno)\n",
    "        predict.append(prediction[0])\n",
    "    return original,predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c9da79a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On subject 0\n",
      "On subject 1\n",
      "On subject 2\n",
      "On subject 3\n",
      "On subject 4\n",
      "On subject 5\n",
      "On subject 6\n",
      "On subject 7\n",
      "On subject 8\n",
      "On subject 9\n",
      "On subject 10\n",
      "On subject 11\n",
      "On subject 12\n",
      "On subject 13\n",
      "On subject 14\n",
      "On subject 15\n",
      "On subject 16\n",
      "On subject 17\n",
      "On subject 18\n",
      "On subject 19\n",
      "On subject 20\n",
      "On subject 21\n",
      "On subject 22\n",
      "On subject 23\n",
      "On subject 24\n",
      "On subject 25\n",
      "On subject 26\n",
      "On subject 27\n",
      "On subject 28\n",
      "On subject 29\n",
      "On subject 30\n",
      "On subject 31\n",
      "On subject 32\n",
      "On subject 33\n",
      "On subject 34\n",
      "On subject 35\n",
      "On subject 36\n",
      "On subject 37\n",
      "On subject 38\n",
      "On subject 39\n",
      "On subject 40\n",
      "On subject 41\n",
      "On subject 42\n",
      "On subject 43\n",
      "On subject 44\n",
      "On subject 45\n",
      "On subject 46\n",
      "On subject 47\n",
      "On subject 48\n",
      "On subject 49\n",
      "On subject 50\n",
      "On subject 51\n",
      "On subject 52\n",
      "On subject 53\n",
      "On subject 54\n",
      "On subject 55\n",
      "On subject 56\n",
      "On subject 57\n",
      "On subject 58\n",
      "On subject 59\n",
      "On subject 60\n",
      "On subject 61\n",
      "On subject 62\n",
      "On subject 63\n",
      "On subject 64\n",
      "On subject 65\n",
      "On subject 66\n",
      "On subject 67\n",
      "On subject 68\n",
      "On subject 69\n",
      "On subject 70\n",
      "On subject 71\n",
      "On subject 72\n",
      "On subject 73\n",
      "On subject 74\n",
      "On subject 75\n",
      "On subject 76\n",
      "On subject 77\n",
      "On subject 78\n",
      "On subject 79\n",
      "On subject 80\n",
      "On subject 81\n",
      "On subject 82\n",
      "On subject 83\n",
      "On subject 84\n",
      "On subject 85\n",
      "On subject 86\n",
      "On subject 87\n",
      "On subject 88\n",
      "On subject 89\n",
      "On subject 90\n",
      "On subject 91\n",
      "On subject 92\n",
      "On subject 93\n",
      "On subject 94\n",
      "On subject 95\n",
      "On subject 96\n",
      "On subject 97\n",
      "On subject 98\n",
      "On subject 99\n",
      "On subject 100\n",
      "On subject 101\n",
      "On subject 102\n",
      "On subject 103\n",
      "On subject 104\n",
      "On subject 105\n",
      "On subject 106\n",
      "On subject 107\n",
      "On subject 108\n",
      "On subject 109\n",
      "On subject 110\n",
      "On subject 111\n",
      "On subject 112\n",
      "On subject 113\n",
      "On subject 114\n",
      "On subject 115\n",
      "On subject 116\n",
      "On subject 117\n",
      "On subject 118\n",
      "On subject 119\n",
      "On subject 120\n",
      "On subject 121\n",
      "On subject 122\n",
      "On subject 123\n",
      "On subject 124\n",
      "On subject 125\n",
      "On subject 126\n",
      "On subject 127\n",
      "On subject 128\n",
      "On subject 129\n",
      "On subject 130\n",
      "On subject 131\n",
      "On subject 132\n",
      "On subject 133\n",
      "On subject 134\n",
      "On subject 135\n",
      "On subject 136\n",
      "On subject 137\n",
      "On subject 138\n",
      "On subject 139\n",
      "On subject 140\n",
      "On subject 141\n",
      "On subject 142\n",
      "On subject 143\n",
      "On subject 144\n",
      "On subject 145\n",
      "On subject 146\n",
      "On subject 147\n",
      "On subject 148\n",
      "On subject 149\n",
      "On subject 150\n",
      "On subject 151\n",
      "On subject 152\n",
      "On subject 153\n",
      "On subject 154\n",
      "On subject 155\n",
      "On subject 156\n",
      "On subject 157\n",
      "On subject 158\n",
      "On subject 159\n",
      "On subject 160\n",
      "On subject 161\n",
      "On subject 162\n",
      "On subject 163\n",
      "On subject 164\n",
      "On subject 165\n",
      "On subject 166\n",
      "On subject 167\n",
      "On subject 168\n",
      "On subject 169\n",
      "On subject 170\n",
      "On subject 171\n",
      "On subject 172\n",
      "On subject 173\n",
      "On subject 174\n",
      "On subject 175\n",
      "On subject 176\n",
      "On subject 177\n",
      "On subject 178\n",
      "On subject 179\n",
      "On subject 180\n",
      "On subject 181\n",
      "On subject 182\n",
      "On subject 183\n",
      "On subject 184\n",
      "On subject 185\n",
      "On subject 186\n",
      "On subject 187\n",
      "On subject 188\n",
      "On subject 189\n",
      "On subject 190\n",
      "On subject 191\n",
      "On subject 192\n",
      "On subject 193\n",
      "On subject 194\n",
      "On subject 195\n",
      "On subject 196\n",
      "On subject 197\n",
      "On subject 198\n",
      "On subject 199\n",
      "On subject 200\n",
      "On subject 201\n",
      "On subject 202\n",
      "On subject 203\n",
      "On subject 204\n",
      "On subject 205\n",
      "On subject 206\n",
      "On subject 207\n",
      "On subject 208\n",
      "On subject 209\n",
      "On subject 210\n",
      "On subject 211\n",
      "On subject 212\n",
      "On subject 213\n",
      "On subject 214\n",
      "On subject 215\n",
      "On subject 216\n",
      "On subject 217\n",
      "On subject 218\n",
      "On subject 219\n",
      "On subject 220\n",
      "On subject 221\n",
      "On subject 222\n",
      "On subject 223\n",
      "On subject 224\n",
      "On subject 225\n",
      "On subject 226\n",
      "On subject 227\n",
      "On subject 228\n",
      "On subject 229\n",
      "On subject 230\n",
      "On subject 231\n",
      "On subject 232\n",
      "On subject 233\n",
      "On subject 234\n",
      "On subject 235\n",
      "On subject 236\n",
      "On subject 237\n",
      "On subject 238\n",
      "On subject 239\n",
      "On subject 240\n",
      "On subject 241\n",
      "On subject 242\n",
      "On subject 243\n",
      "On subject 244\n",
      "On subject 245\n",
      "On subject 246\n",
      "On subject 247\n",
      "On subject 248\n",
      "On subject 249\n",
      "On subject 250\n",
      "On subject 251\n",
      "On subject 252\n",
      "On subject 253\n",
      "On subject 254\n",
      "On subject 255\n",
      "On subject 256\n",
      "On subject 257\n",
      "On subject 258\n",
      "On subject 259\n",
      "On subject 260\n",
      "On subject 261\n",
      "On subject 262\n",
      "On subject 263\n",
      "On subject 264\n",
      "On subject 265\n",
      "On subject 266\n",
      "On subject 267\n",
      "On subject 268\n",
      "On subject 269\n",
      "On subject 270\n",
      "On subject 271\n",
      "On subject 272\n",
      "On subject 273\n",
      "On subject 274\n",
      "On subject 275\n",
      "On subject 276\n",
      "On subject 277\n",
      "On subject 278\n",
      "On subject 279\n",
      "On subject 280\n",
      "On subject 281\n",
      "On subject 282\n",
      "On subject 283\n",
      "On subject 284\n",
      "On subject 285\n",
      "On subject 286\n",
      "On subject 287\n",
      "On subject 288\n",
      "On subject 289\n",
      "On subject 290\n",
      "On subject 291\n",
      "On subject 292\n",
      "On subject 293\n",
      "On subject 294\n",
      "On subject 295\n",
      "On subject 296\n",
      "On subject 297\n",
      "On subject 298\n",
      "On subject 299\n",
      "On subject 300\n",
      "On subject 301\n",
      "On subject 302\n",
      "On subject 303\n",
      "On subject 304\n",
      "On subject 305\n",
      "On subject 306\n",
      "On subject 307\n",
      "On subject 308\n",
      "On subject 309\n",
      "On subject 310\n",
      "On subject 311\n",
      "On subject 312\n",
      "On subject 313\n",
      "On subject 314\n",
      "On subject 315\n",
      "Correlation of the predicted and original score is 0.5894942746983067.\n"
     ]
    }
   ],
   "source": [
    "original, predict = ridgeRegression(sub_sum_pos,sub_sum_neg,pheno)\n",
    "cor = np.corrcoef(np.asarray(original),np.asarray(predict))[0][1]\n",
    "print(\"Correlation of the predicted and original score is %s.\" %cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c0a506",
   "metadata": {},
   "source": [
    "3. Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d2b7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "def lassoRegression(sub_sum_pos,sub_sum_neg,pheno):\n",
    "    \n",
    "    test_image = []\n",
    "    original = []\n",
    "    predict = []\n",
    "    \n",
    "    for i in range(np.shape(sub_sum_pos)[0]):\n",
    "        print(\"On subject %s\" %i)\n",
    "        loo_sub_sum_pos = np.delete(sub_sum_pos,[i])\n",
    "        single_sub_sum_pos = sub_sum_pos[i]\n",
    "        loo_sub_sum_neg = np.delete(sub_sum_neg,[i])\n",
    "        single_sub_sum_neg = sub_sum_neg[i]\n",
    "        loo_pheno = np.delete(pheno,[i])\n",
    "        single_sub_pheno = pheno[i]\n",
    "        X_train = np.vstack((np.reshape(loo_sub_sum_pos,(1,315)), np.reshape(loo_sub_sum_neg,(1,315))))\n",
    "        X_train = np.transpose(X_train)\n",
    "        Y_train = loo_pheno\n",
    "        reg = Lasso(alpha = 1.0).fit(X_train, Y_train)\n",
    "        test_x = np.transpose(np.vstack((single_sub_sum_pos,single_sub_sum_neg)))\n",
    "        prediction = reg.predict(test_x)\n",
    "        test_image.append(test_x)\n",
    "        original.append(single_sub_pheno)\n",
    "        predict.append(prediction[0])\n",
    "    \n",
    "    return original,predict,test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb5ef25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On subject 0\n",
      "On subject 1\n",
      "On subject 2\n",
      "On subject 3\n",
      "On subject 4\n",
      "On subject 5\n",
      "On subject 6\n",
      "On subject 7\n",
      "On subject 8\n",
      "On subject 9\n",
      "On subject 10\n",
      "On subject 11\n",
      "On subject 12\n",
      "On subject 13\n",
      "On subject 14\n",
      "On subject 15\n",
      "On subject 16\n",
      "On subject 17\n",
      "On subject 18\n",
      "On subject 19\n",
      "On subject 20\n",
      "On subject 21\n",
      "On subject 22\n",
      "On subject 23\n",
      "On subject 24\n",
      "On subject 25\n",
      "On subject 26\n",
      "On subject 27\n",
      "On subject 28\n",
      "On subject 29\n",
      "On subject 30\n",
      "On subject 31\n",
      "On subject 32\n",
      "On subject 33\n",
      "On subject 34\n",
      "On subject 35\n",
      "On subject 36\n",
      "On subject 37\n",
      "On subject 38\n",
      "On subject 39\n",
      "On subject 40\n",
      "On subject 41\n",
      "On subject 42\n",
      "On subject 43\n",
      "On subject 44\n",
      "On subject 45\n",
      "On subject 46\n",
      "On subject 47\n",
      "On subject 48\n",
      "On subject 49\n",
      "On subject 50\n",
      "On subject 51\n",
      "On subject 52\n",
      "On subject 53\n",
      "On subject 54\n",
      "On subject 55\n",
      "On subject 56\n",
      "On subject 57\n",
      "On subject 58\n",
      "On subject 59\n",
      "On subject 60\n",
      "On subject 61\n",
      "On subject 62\n",
      "On subject 63\n",
      "On subject 64\n",
      "On subject 65\n",
      "On subject 66\n",
      "On subject 67\n",
      "On subject 68\n",
      "On subject 69\n",
      "On subject 70\n",
      "On subject 71\n",
      "On subject 72\n",
      "On subject 73\n",
      "On subject 74\n",
      "On subject 75\n",
      "On subject 76\n",
      "On subject 77\n",
      "On subject 78\n",
      "On subject 79\n",
      "On subject 80\n",
      "On subject 81\n",
      "On subject 82\n",
      "On subject 83\n",
      "On subject 84\n",
      "On subject 85\n",
      "On subject 86\n",
      "On subject 87\n",
      "On subject 88\n",
      "On subject 89\n",
      "On subject 90\n",
      "On subject 91\n",
      "On subject 92\n",
      "On subject 93\n",
      "On subject 94\n",
      "On subject 95\n",
      "On subject 96\n",
      "On subject 97\n",
      "On subject 98\n",
      "On subject 99\n",
      "On subject 100\n",
      "On subject 101\n",
      "On subject 102\n",
      "On subject 103\n",
      "On subject 104\n",
      "On subject 105\n",
      "On subject 106\n",
      "On subject 107\n",
      "On subject 108\n",
      "On subject 109\n",
      "On subject 110\n",
      "On subject 111\n",
      "On subject 112\n",
      "On subject 113\n",
      "On subject 114\n",
      "On subject 115\n",
      "On subject 116\n",
      "On subject 117\n",
      "On subject 118\n",
      "On subject 119\n",
      "On subject 120\n",
      "On subject 121\n",
      "On subject 122\n",
      "On subject 123\n",
      "On subject 124\n",
      "On subject 125\n",
      "On subject 126\n",
      "On subject 127\n",
      "On subject 128\n",
      "On subject 129\n",
      "On subject 130\n",
      "On subject 131\n",
      "On subject 132\n",
      "On subject 133\n",
      "On subject 134\n",
      "On subject 135\n",
      "On subject 136\n",
      "On subject 137\n",
      "On subject 138\n",
      "On subject 139\n",
      "On subject 140\n",
      "On subject 141\n",
      "On subject 142\n",
      "On subject 143\n",
      "On subject 144\n",
      "On subject 145\n",
      "On subject 146\n",
      "On subject 147\n",
      "On subject 148\n",
      "On subject 149\n",
      "On subject 150\n",
      "On subject 151\n",
      "On subject 152\n",
      "On subject 153\n",
      "On subject 154\n",
      "On subject 155\n",
      "On subject 156\n",
      "On subject 157\n",
      "On subject 158\n",
      "On subject 159\n",
      "On subject 160\n",
      "On subject 161\n",
      "On subject 162\n",
      "On subject 163\n",
      "On subject 164\n",
      "On subject 165\n",
      "On subject 166\n",
      "On subject 167\n",
      "On subject 168\n",
      "On subject 169\n",
      "On subject 170\n",
      "On subject 171\n",
      "On subject 172\n",
      "On subject 173\n",
      "On subject 174\n",
      "On subject 175\n",
      "On subject 176\n",
      "On subject 177\n",
      "On subject 178\n",
      "On subject 179\n",
      "On subject 180\n",
      "On subject 181\n",
      "On subject 182\n",
      "On subject 183\n",
      "On subject 184\n",
      "On subject 185\n",
      "On subject 186\n",
      "On subject 187\n",
      "On subject 188\n",
      "On subject 189\n",
      "On subject 190\n",
      "On subject 191\n",
      "On subject 192\n",
      "On subject 193\n",
      "On subject 194\n",
      "On subject 195\n",
      "On subject 196\n",
      "On subject 197\n",
      "On subject 198\n",
      "On subject 199\n",
      "On subject 200\n",
      "On subject 201\n",
      "On subject 202\n",
      "On subject 203\n",
      "On subject 204\n",
      "On subject 205\n",
      "On subject 206\n",
      "On subject 207\n",
      "On subject 208\n",
      "On subject 209\n",
      "On subject 210\n",
      "On subject 211\n",
      "On subject 212\n",
      "On subject 213\n",
      "On subject 214\n",
      "On subject 215\n",
      "On subject 216\n",
      "On subject 217\n",
      "On subject 218\n",
      "On subject 219\n",
      "On subject 220\n",
      "On subject 221\n",
      "On subject 222\n",
      "On subject 223\n",
      "On subject 224\n",
      "On subject 225\n",
      "On subject 226\n",
      "On subject 227\n",
      "On subject 228\n",
      "On subject 229\n",
      "On subject 230\n",
      "On subject 231\n",
      "On subject 232\n",
      "On subject 233\n",
      "On subject 234\n",
      "On subject 235\n",
      "On subject 236\n",
      "On subject 237\n",
      "On subject 238\n",
      "On subject 239\n",
      "On subject 240\n",
      "On subject 241\n",
      "On subject 242\n",
      "On subject 243\n",
      "On subject 244\n",
      "On subject 245\n",
      "On subject 246\n",
      "On subject 247\n",
      "On subject 248\n",
      "On subject 249\n",
      "On subject 250\n",
      "On subject 251\n",
      "On subject 252\n",
      "On subject 253\n",
      "On subject 254\n",
      "On subject 255\n",
      "On subject 256\n",
      "On subject 257\n",
      "On subject 258\n",
      "On subject 259\n",
      "On subject 260\n",
      "On subject 261\n",
      "On subject 262\n",
      "On subject 263\n",
      "On subject 264\n",
      "On subject 265\n",
      "On subject 266\n",
      "On subject 267\n",
      "On subject 268\n",
      "On subject 269\n",
      "On subject 270\n",
      "On subject 271\n",
      "On subject 272\n",
      "On subject 273\n",
      "On subject 274\n",
      "On subject 275\n",
      "On subject 276\n",
      "On subject 277\n",
      "On subject 278\n",
      "On subject 279\n",
      "On subject 280\n",
      "On subject 281\n",
      "On subject 282\n",
      "On subject 283\n",
      "On subject 284\n",
      "On subject 285\n",
      "On subject 286\n",
      "On subject 287\n",
      "On subject 288\n",
      "On subject 289\n",
      "On subject 290\n",
      "On subject 291\n",
      "On subject 292\n",
      "On subject 293\n",
      "On subject 294\n",
      "On subject 295\n",
      "On subject 296\n",
      "On subject 297\n",
      "On subject 298\n",
      "On subject 299\n",
      "On subject 300\n",
      "On subject 301\n",
      "On subject 302\n",
      "On subject 303\n",
      "On subject 304\n",
      "On subject 305\n",
      "On subject 306\n",
      "On subject 307\n",
      "On subject 308\n",
      "On subject 309\n",
      "On subject 310\n",
      "On subject 311\n",
      "On subject 312\n",
      "On subject 313\n",
      "On subject 314\n",
      "On subject 315\n",
      "Correlation of the predicted and original score is 0.588768863323186.\n"
     ]
    }
   ],
   "source": [
    "original, predict, test_image = lassoRegression(sub_sum_pos,sub_sum_neg,pheno)\n",
    "cor = np.corrcoef(np.asarray(original),np.asarray(predict))[0][1]\n",
    "print(\"Correlation of the predicted and original score is %s.\" %cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4a1b3",
   "metadata": {},
   "source": [
    "4. Elastic-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "b458bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "def elasticNet(sub_sum_pos,sub_sum_neg,pheno):\n",
    "    \n",
    "    test_image = []\n",
    "    original = []\n",
    "    predict = []\n",
    "    \n",
    "    for i in range(np.shape(sub_sum_pos)[0]):\n",
    "        print(\"On subject %s\" %i)\n",
    "        loo_sub_sum_pos = np.delete(sub_sum_pos,[i])\n",
    "        single_sub_sum_pos = sub_sum_pos[i]\n",
    "        loo_sub_sum_neg = np.delete(sub_sum_neg,[i])\n",
    "        single_sub_sum_neg = sub_sum_neg[i]\n",
    "        loo_pheno = np.delete(pheno,[i])\n",
    "        single_sub_pheno = pheno[i]\n",
    "        print(np.shape(loo_sub_sum_pos))\n",
    "        X_train = np.vstack((np.reshape(loo_sub_sum_pos,(1,315)), np.reshape(loo_sub_sum_neg,(1,315))))\n",
    "        X_train = np.transpose(X_train)\n",
    "        Y_train = loo_pheno\n",
    "        reg = ElasticNet(alpha = 1.0).fit(X_train, Y_train)\n",
    "        test_x = np.transpose(np.vstack((single_sub_sum_pos,single_sub_sum_neg)))\n",
    "        prediction = reg.predict(test_x)\n",
    "        test_image.append(test_x)\n",
    "        original.append(single_sub_pheno)\n",
    "        predict.append(prediction[0])\n",
    "    \n",
    "    return original,predict,test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3d0b6904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On subject 0\n",
      "(84687,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 84687 into shape (1,315)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-8037a9a6a002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melasticNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_sum_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_sum_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpheno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Correlation of the predicted and original score is %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mcor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-215-436010f7efc7>\u001b[0m in \u001b[0;36melasticNet\u001b[0;34m(sub_sum_pos, sub_sum_neg, pheno)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msingle_sub_pheno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpheno\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloo_sub_sum_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloo_sub_sum_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m315\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloo_sub_sum_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m315\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloo_pheno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    297\u001b[0m            [5, 6]])\n\u001b[1;32m    298\u001b[0m     \"\"\"\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 84687 into shape (1,315)"
     ]
    }
   ],
   "source": [
    "original, predict, test_image = elasticNet(sub_sum_pos,sub_sum_neg,pheno)\n",
    "cor = np.corrcoef(np.asarray(original),np.asarray(predict))[0][1]\n",
    "print(\"Correlation of the predicted and original score is %s.\" %cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b08c78",
   "metadata": {},
   "source": [
    "5. LARS-lasso\n",
    "\n",
    "Note: It is a Linear Model trained with an L1 prior as regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5184a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "def lassoLars(sub_sum_pos,sub_sum_neg,pheno):\n",
    "    \n",
    "    test_image = []\n",
    "    original = []\n",
    "    predict = []\n",
    "    \n",
    "    for i in range(np.shape(sub_sum_pos)[0]):\n",
    "        print(\"On subject %s\" %i)\n",
    "        loo_sub_sum_pos = np.delete(sub_sum_pos,[i])\n",
    "        single_sub_sum_pos = sub_sum_pos[i]\n",
    "        loo_sub_sum_neg = np.delete(sub_sum_neg,[i])\n",
    "        single_sub_sum_neg = sub_sum_neg[i]\n",
    "        loo_pheno = np.delete(pheno,[i])\n",
    "        single_sub_pheno = pheno[i]\n",
    "        X_train = np.vstack((np.reshape(loo_sub_sum_pos,(1,315)), np.reshape(loo_sub_sum_neg,(1,315))))\n",
    "        X_train = np.transpose(X_train)\n",
    "        Y_train = loo_pheno\n",
    "        reg = LassoLars(alpha = 1.0).fit(X_train, Y_train)\n",
    "        test_x = np.transpose(np.vstack((single_sub_sum_pos,single_sub_sum_neg)))\n",
    "        prediction = reg.predict(test_x)\n",
    "        test_image.append(test_x)\n",
    "        original.append(single_sub_pheno)\n",
    "        predict.append(prediction[0])\n",
    "    \n",
    "    return original,predict,test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "db1fea88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On subject 0\n",
      "On subject 1\n",
      "On subject 2\n",
      "On subject 3\n",
      "On subject 4\n",
      "On subject 5\n",
      "On subject 6\n",
      "On subject 7\n",
      "On subject 8\n",
      "On subject 9\n",
      "On subject 10\n",
      "On subject 11\n",
      "On subject 12\n",
      "On subject 13\n",
      "On subject 14\n",
      "On subject 15\n",
      "On subject 16\n",
      "On subject 17\n",
      "On subject 18\n",
      "On subject 19\n",
      "On subject 20\n",
      "On subject 21\n",
      "On subject 22\n",
      "On subject 23\n",
      "On subject 24\n",
      "On subject 25\n",
      "On subject 26\n",
      "On subject 27\n",
      "On subject 28\n",
      "On subject 29\n",
      "On subject 30\n",
      "On subject 31\n",
      "On subject 32\n",
      "On subject 33\n",
      "On subject 34\n",
      "On subject 35\n",
      "On subject 36\n",
      "On subject 37\n",
      "On subject 38\n",
      "On subject 39\n",
      "On subject 40\n",
      "On subject 41\n",
      "On subject 42\n",
      "On subject 43\n",
      "On subject 44\n",
      "On subject 45\n",
      "On subject 46\n",
      "On subject 47\n",
      "On subject 48\n",
      "On subject 49\n",
      "On subject 50\n",
      "On subject 51\n",
      "On subject 52\n",
      "On subject 53\n",
      "On subject 54\n",
      "On subject 55\n",
      "On subject 56\n",
      "On subject 57\n",
      "On subject 58\n",
      "On subject 59\n",
      "On subject 60\n",
      "On subject 61\n",
      "On subject 62\n",
      "On subject 63\n",
      "On subject 64\n",
      "On subject 65\n",
      "On subject 66\n",
      "On subject 67\n",
      "On subject 68\n",
      "On subject 69\n",
      "On subject 70\n",
      "On subject 71\n",
      "On subject 72\n",
      "On subject 73\n",
      "On subject 74\n",
      "On subject 75\n",
      "On subject 76\n",
      "On subject 77\n",
      "On subject 78\n",
      "On subject 79\n",
      "On subject 80\n",
      "On subject 81\n",
      "On subject 82\n",
      "On subject 83\n",
      "On subject 84\n",
      "On subject 85\n",
      "On subject 86\n",
      "On subject 87\n",
      "On subject 88\n",
      "On subject 89\n",
      "On subject 90\n",
      "On subject 91\n",
      "On subject 92\n",
      "On subject 93\n",
      "On subject 94\n",
      "On subject 95\n",
      "On subject 96\n",
      "On subject 97\n",
      "On subject 98\n",
      "On subject 99\n",
      "On subject 100\n",
      "On subject 101\n",
      "On subject 102\n",
      "On subject 103\n",
      "On subject 104\n",
      "On subject 105\n",
      "On subject 106\n",
      "On subject 107\n",
      "On subject 108\n",
      "On subject 109\n",
      "On subject 110\n",
      "On subject 111\n",
      "On subject 112\n",
      "On subject 113\n",
      "On subject 114\n",
      "On subject 115\n",
      "On subject 116\n",
      "On subject 117\n",
      "On subject 118\n",
      "On subject 119\n",
      "On subject 120\n",
      "On subject 121\n",
      "On subject 122\n",
      "On subject 123\n",
      "On subject 124\n",
      "On subject 125\n",
      "On subject 126\n",
      "On subject 127\n",
      "On subject 128\n",
      "On subject 129\n",
      "On subject 130\n",
      "On subject 131\n",
      "On subject 132\n",
      "On subject 133\n",
      "On subject 134\n",
      "On subject 135\n",
      "On subject 136\n",
      "On subject 137\n",
      "On subject 138\n",
      "On subject 139\n",
      "On subject 140\n",
      "On subject 141\n",
      "On subject 142\n",
      "On subject 143\n",
      "On subject 144\n",
      "On subject 145\n",
      "On subject 146\n",
      "On subject 147\n",
      "On subject 148\n",
      "On subject 149\n",
      "On subject 150\n",
      "On subject 151\n",
      "On subject 152\n",
      "On subject 153\n",
      "On subject 154\n",
      "On subject 155\n",
      "On subject 156\n",
      "On subject 157\n",
      "On subject 158\n",
      "On subject 159\n",
      "On subject 160\n",
      "On subject 161\n",
      "On subject 162\n",
      "On subject 163\n",
      "On subject 164\n",
      "On subject 165\n",
      "On subject 166\n",
      "On subject 167\n",
      "On subject 168\n",
      "On subject 169\n",
      "On subject 170\n",
      "On subject 171\n",
      "On subject 172\n",
      "On subject 173\n",
      "On subject 174\n",
      "On subject 175\n",
      "On subject 176\n",
      "On subject 177\n",
      "On subject 178\n",
      "On subject 179\n",
      "On subject 180\n",
      "On subject 181\n",
      "On subject 182\n",
      "On subject 183\n",
      "On subject 184\n",
      "On subject 185\n",
      "On subject 186\n",
      "On subject 187\n",
      "On subject 188\n",
      "On subject 189\n",
      "On subject 190\n",
      "On subject 191\n",
      "On subject 192\n",
      "On subject 193\n",
      "On subject 194\n",
      "On subject 195\n",
      "On subject 196\n",
      "On subject 197\n",
      "On subject 198\n",
      "On subject 199\n",
      "On subject 200\n",
      "On subject 201\n",
      "On subject 202\n",
      "On subject 203\n",
      "On subject 204\n",
      "On subject 205\n",
      "On subject 206\n",
      "On subject 207\n",
      "On subject 208\n",
      "On subject 209\n",
      "On subject 210\n",
      "On subject 211\n",
      "On subject 212\n",
      "On subject 213\n",
      "On subject 214\n",
      "On subject 215\n",
      "On subject 216\n",
      "On subject 217\n",
      "On subject 218\n",
      "On subject 219\n",
      "On subject 220\n",
      "On subject 221\n",
      "On subject 222\n",
      "On subject 223\n",
      "On subject 224\n",
      "On subject 225\n",
      "On subject 226\n",
      "On subject 227\n",
      "On subject 228\n",
      "On subject 229\n",
      "On subject 230\n",
      "On subject 231\n",
      "On subject 232\n",
      "On subject 233\n",
      "On subject 234\n",
      "On subject 235\n",
      "On subject 236\n",
      "On subject 237\n",
      "On subject 238\n",
      "On subject 239\n",
      "On subject 240\n",
      "On subject 241\n",
      "On subject 242\n",
      "On subject 243\n",
      "On subject 244\n",
      "On subject 245\n",
      "On subject 246\n",
      "On subject 247\n",
      "On subject 248\n",
      "On subject 249\n",
      "On subject 250\n",
      "On subject 251\n",
      "On subject 252\n",
      "On subject 253\n",
      "On subject 254\n",
      "On subject 255\n",
      "On subject 256\n",
      "On subject 257\n",
      "On subject 258\n",
      "On subject 259\n",
      "On subject 260\n",
      "On subject 261\n",
      "On subject 262\n",
      "On subject 263\n",
      "On subject 264\n",
      "On subject 265\n",
      "On subject 266\n",
      "On subject 267\n",
      "On subject 268\n",
      "On subject 269\n",
      "On subject 270\n",
      "On subject 271\n",
      "On subject 272\n",
      "On subject 273\n",
      "On subject 274\n",
      "On subject 275\n",
      "On subject 276\n",
      "On subject 277\n",
      "On subject 278\n",
      "On subject 279\n",
      "On subject 280\n",
      "On subject 281\n",
      "On subject 282\n",
      "On subject 283\n",
      "On subject 284\n",
      "On subject 285\n",
      "On subject 286\n",
      "On subject 287\n",
      "On subject 288\n",
      "On subject 289\n",
      "On subject 290\n",
      "On subject 291\n",
      "On subject 292\n",
      "On subject 293\n",
      "On subject 294\n",
      "On subject 295\n",
      "On subject 296\n",
      "On subject 297\n",
      "On subject 298\n",
      "On subject 299\n",
      "On subject 300\n",
      "On subject 301\n",
      "On subject 302\n",
      "On subject 303\n",
      "On subject 304\n",
      "On subject 305\n",
      "On subject 306\n",
      "On subject 307\n",
      "On subject 308\n",
      "On subject 309\n",
      "On subject 310\n",
      "On subject 311\n",
      "On subject 312\n",
      "On subject 313\n",
      "On subject 314\n",
      "On subject 315\n",
      "Correlation of the predicted and original score is -1.0.\n"
     ]
    }
   ],
   "source": [
    "original, predict, test_image = lassoLars(sub_sum_pos,sub_sum_neg,pheno)\n",
    "cor = np.corrcoef(np.asarray(original),np.asarray(predict))[0][1]\n",
    "print(\"Correlation of the predicted and original score is %s.\" %cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ae16a",
   "metadata": {},
   "source": [
    "*plot to see whether the prediction and original are perfectly negatively correlated*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cbd66154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "dataFrame = pd.DataFrame(\n",
    "    {'original': original,\n",
    "     'prediction': predict\n",
    "    }) #create a dataframe for the data\n",
    "ax = sns.scatterplot(data=dataFrame, x=\"original\", y=\"prediction\")\n",
    "ax.set_title('Relationship between original and prediction using LassoLars',\n",
    "             fontdict= { 'fontsize': 15, 'fontweight':'bold'});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fcacde",
   "metadata": {},
   "source": [
    "6. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a519a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def SGDRegress(sub_sum_pos,sub_sum_neg,pheno):\n",
    "    \n",
    "    test_image = []\n",
    "    original = []\n",
    "    predict = []\n",
    "    \n",
    "    for i in range(np.shape(sub_sum_pos)[0]):\n",
    "        print(\"On subject %s\" %i)\n",
    "        loo_sub_sum_pos = np.delete(sub_sum_pos,[i])\n",
    "        single_sub_sum_pos = sub_sum_pos[i]\n",
    "        loo_sub_sum_neg = np.delete(sub_sum_neg,[i])\n",
    "        single_sub_sum_neg = sub_sum_neg[i]\n",
    "        loo_pheno = np.delete(pheno,[i])\n",
    "        single_sub_pheno = pheno[i]\n",
    "        X_train = np.vstack((np.reshape(loo_sub_sum_pos,(1,315)), np.reshape(loo_sub_sum_neg,(1,315))))\n",
    "        X_train = np.transpose(X_train)\n",
    "        Y_train = loo_pheno\n",
    "        reg = make_pipeline(StandardScaler(), SGDRegressor(alpha=1.0)).fit(X_train, Y_train)\n",
    "        '''\n",
    "        Args\n",
    "        \n",
    "        1. loss\n",
    "        The ‘squared_loss’ refers to the ordinary least squares fit. \n",
    "        ‘huber’ modifies ‘squared_loss’ to focus less on getting outliers correct by switching from \n",
    "        squared to linear loss past a distance of epsilon. ‘epsilon_insensitive’ ignores errors less \n",
    "        than epsilon and is linear past that; this is the loss function used in SVR. \n",
    "        ‘squared_epsilon_insensitive’ is the same but becomes squared loss past a tolerance of epsilon.\n",
    "        \n",
    "        2. penalty\n",
    "        The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer \n",
    "        for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not \n",
    "        achievable with ‘l2’.\n",
    "        \n",
    "        3. alphafloat, default=0.0001\n",
    "        Constant that multiplies the regularization term. The higher the value, the stronger the regularization. \n",
    "        Also used to compute the learning rate when set to learning_rate is set to ‘optimal’.\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        test_x = np.transpose(np.vstack((single_sub_sum_pos,single_sub_sum_neg)))\n",
    "        prediction = reg.predict(test_x)\n",
    "        test_image.append(test_x)\n",
    "        original.append(single_sub_pheno)\n",
    "        predict.append(prediction[0])\n",
    "    \n",
    "    return original,predict,test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f42e971a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On subject 0\n",
      "On subject 1\n",
      "On subject 2\n",
      "On subject 3\n",
      "On subject 4\n",
      "On subject 5\n",
      "On subject 6\n",
      "On subject 7\n",
      "On subject 8\n",
      "On subject 9\n",
      "On subject 10\n",
      "On subject 11\n",
      "On subject 12\n",
      "On subject 13\n",
      "On subject 14\n",
      "On subject 15\n",
      "On subject 16\n",
      "On subject 17\n",
      "On subject 18\n",
      "On subject 19\n",
      "On subject 20\n",
      "On subject 21\n",
      "On subject 22\n",
      "On subject 23\n",
      "On subject 24\n",
      "On subject 25\n",
      "On subject 26\n",
      "On subject 27\n",
      "On subject 28\n",
      "On subject 29\n",
      "On subject 30\n",
      "On subject 31\n",
      "On subject 32\n",
      "On subject 33\n",
      "On subject 34\n",
      "On subject 35\n",
      "On subject 36\n",
      "On subject 37\n",
      "On subject 38\n",
      "On subject 39\n",
      "On subject 40\n",
      "On subject 41\n",
      "On subject 42\n",
      "On subject 43\n",
      "On subject 44\n",
      "On subject 45\n",
      "On subject 46\n",
      "On subject 47\n",
      "On subject 48\n",
      "On subject 49\n",
      "On subject 50\n",
      "On subject 51\n",
      "On subject 52\n",
      "On subject 53\n",
      "On subject 54\n",
      "On subject 55\n",
      "On subject 56\n",
      "On subject 57\n",
      "On subject 58\n",
      "On subject 59\n",
      "On subject 60\n",
      "On subject 61\n",
      "On subject 62\n",
      "On subject 63\n",
      "On subject 64\n",
      "On subject 65\n",
      "On subject 66\n",
      "On subject 67\n",
      "On subject 68\n",
      "On subject 69\n",
      "On subject 70\n",
      "On subject 71\n",
      "On subject 72\n",
      "On subject 73\n",
      "On subject 74\n",
      "On subject 75\n",
      "On subject 76\n",
      "On subject 77\n",
      "On subject 78\n",
      "On subject 79\n",
      "On subject 80\n",
      "On subject 81\n",
      "On subject 82\n",
      "On subject 83\n",
      "On subject 84\n",
      "On subject 85\n",
      "On subject 86\n",
      "On subject 87\n",
      "On subject 88\n",
      "On subject 89\n",
      "On subject 90\n",
      "On subject 91\n",
      "On subject 92\n",
      "On subject 93\n",
      "On subject 94\n",
      "On subject 95\n",
      "On subject 96\n",
      "On subject 97\n",
      "On subject 98\n",
      "On subject 99\n",
      "On subject 100\n",
      "On subject 101\n",
      "On subject 102\n",
      "On subject 103\n",
      "On subject 104\n",
      "On subject 105\n",
      "On subject 106\n",
      "On subject 107\n",
      "On subject 108\n",
      "On subject 109\n",
      "On subject 110\n",
      "On subject 111\n",
      "On subject 112\n",
      "On subject 113\n",
      "On subject 114\n",
      "On subject 115\n",
      "On subject 116\n",
      "On subject 117\n",
      "On subject 118\n",
      "On subject 119\n",
      "On subject 120\n",
      "On subject 121\n",
      "On subject 122\n",
      "On subject 123\n",
      "On subject 124\n",
      "On subject 125\n",
      "On subject 126\n",
      "On subject 127\n",
      "On subject 128\n",
      "On subject 129\n",
      "On subject 130\n",
      "On subject 131\n",
      "On subject 132\n",
      "On subject 133\n",
      "On subject 134\n",
      "On subject 135\n",
      "On subject 136\n",
      "On subject 137\n",
      "On subject 138\n",
      "On subject 139\n",
      "On subject 140\n",
      "On subject 141\n",
      "On subject 142\n",
      "On subject 143\n",
      "On subject 144\n",
      "On subject 145\n",
      "On subject 146\n",
      "On subject 147\n",
      "On subject 148\n",
      "On subject 149\n",
      "On subject 150\n",
      "On subject 151\n",
      "On subject 152\n",
      "On subject 153\n",
      "On subject 154\n",
      "On subject 155\n",
      "On subject 156\n",
      "On subject 157\n",
      "On subject 158\n",
      "On subject 159\n",
      "On subject 160\n",
      "On subject 161\n",
      "On subject 162\n",
      "On subject 163\n",
      "On subject 164\n",
      "On subject 165\n",
      "On subject 166\n",
      "On subject 167\n",
      "On subject 168\n",
      "On subject 169\n",
      "On subject 170\n",
      "On subject 171\n",
      "On subject 172\n",
      "On subject 173\n",
      "On subject 174\n",
      "On subject 175\n",
      "On subject 176\n",
      "On subject 177\n",
      "On subject 178\n",
      "On subject 179\n",
      "On subject 180\n",
      "On subject 181\n",
      "On subject 182\n",
      "On subject 183\n",
      "On subject 184\n",
      "On subject 185\n",
      "On subject 186\n",
      "On subject 187\n",
      "On subject 188\n",
      "On subject 189\n",
      "On subject 190\n",
      "On subject 191\n",
      "On subject 192\n",
      "On subject 193\n",
      "On subject 194\n",
      "On subject 195\n",
      "On subject 196\n",
      "On subject 197\n",
      "On subject 198\n",
      "On subject 199\n",
      "On subject 200\n",
      "On subject 201\n",
      "On subject 202\n",
      "On subject 203\n",
      "On subject 204\n",
      "On subject 205\n",
      "On subject 206\n",
      "On subject 207\n",
      "On subject 208\n",
      "On subject 209\n",
      "On subject 210\n",
      "On subject 211\n",
      "On subject 212\n",
      "On subject 213\n",
      "On subject 214\n",
      "On subject 215\n",
      "On subject 216\n",
      "On subject 217\n",
      "On subject 218\n",
      "On subject 219\n",
      "On subject 220\n",
      "On subject 221\n",
      "On subject 222\n",
      "On subject 223\n",
      "On subject 224\n",
      "On subject 225\n",
      "On subject 226\n",
      "On subject 227\n",
      "On subject 228\n",
      "On subject 229\n",
      "On subject 230\n",
      "On subject 231\n",
      "On subject 232\n",
      "On subject 233\n",
      "On subject 234\n",
      "On subject 235\n",
      "On subject 236\n",
      "On subject 237\n",
      "On subject 238\n",
      "On subject 239\n",
      "On subject 240\n",
      "On subject 241\n",
      "On subject 242\n",
      "On subject 243\n",
      "On subject 244\n",
      "On subject 245\n",
      "On subject 246\n",
      "On subject 247\n",
      "On subject 248\n",
      "On subject 249\n",
      "On subject 250\n",
      "On subject 251\n",
      "On subject 252\n",
      "On subject 253\n",
      "On subject 254\n",
      "On subject 255\n",
      "On subject 256\n",
      "On subject 257\n",
      "On subject 258\n",
      "On subject 259\n",
      "On subject 260\n",
      "On subject 261\n",
      "On subject 262\n",
      "On subject 263\n",
      "On subject 264\n",
      "On subject 265\n",
      "On subject 266\n",
      "On subject 267\n",
      "On subject 268\n",
      "On subject 269\n",
      "On subject 270\n",
      "On subject 271\n",
      "On subject 272\n",
      "On subject 273\n",
      "On subject 274\n",
      "On subject 275\n",
      "On subject 276\n",
      "On subject 277\n",
      "On subject 278\n",
      "On subject 279\n",
      "On subject 280\n",
      "On subject 281\n",
      "On subject 282\n",
      "On subject 283\n",
      "On subject 284\n",
      "On subject 285\n",
      "On subject 286\n",
      "On subject 287\n",
      "On subject 288\n",
      "On subject 289\n",
      "On subject 290\n",
      "On subject 291\n",
      "On subject 292\n",
      "On subject 293\n",
      "On subject 294\n",
      "On subject 295\n",
      "On subject 296\n",
      "On subject 297\n",
      "On subject 298\n",
      "On subject 299\n",
      "On subject 300\n",
      "On subject 301\n",
      "On subject 302\n",
      "On subject 303\n",
      "On subject 304\n",
      "On subject 305\n",
      "On subject 306\n",
      "On subject 307\n",
      "On subject 308\n",
      "On subject 309\n",
      "On subject 310\n",
      "On subject 311\n",
      "On subject 312\n",
      "On subject 313\n",
      "On subject 314\n",
      "On subject 315\n",
      "Correlation of the predicted and original score is 0.5865179031388511.\n"
     ]
    }
   ],
   "source": [
    "original, predict, test_image = SGDRegress(sub_sum_pos,sub_sum_neg,pheno)\n",
    "cor = np.corrcoef(np.asarray(original),np.asarray(predict))[0][1]\n",
    "print(\"Correlation of the predicted and original score is %s.\" %cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2637c17",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c794e19",
   "metadata": {},
   "source": [
    "**Use correlation to evaluate different models with different alpha values (the value that control the regularization/penalty)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8417f",
   "metadata": {},
   "source": [
    "*Correlation of each model with different alpha*\n",
    "\n",
    "|  | LSR | Ridge | Lasso | ElasticNet | LARS-Lasso | Stochastic gradient descent | \n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| alpha = 0.0 | 0.5894942746983071 |0.5894942746983067 | 0.5894942746983067 | 0.5894942746983067 | 0.5894942746983068 | 0.5894944595848238 |\n",
    "| alpha = 0.1 | / | 0.5894942657346615 | 0.5894519058277794 | 0.5894719470529471 | 0.5169165935569177 | 0.5891520624645177 | \n",
    "| alpha = 0.2 | / | 0.5894942567660058 | 0.5894032548024233 | 0.5894471287093305 | -1.0 | 0.5895695339551591 |\n",
    "| alpha = 0.3 | / | 0.5894942477923397 | 0.589348130112169 | 0.5894197471228471 | -1.0 | 0.5887090551304505 |\n",
    "| alpha = 0.4 | / | 0.589494238813663 | 0.5892863911737424 | 0.5893897830994119 | -1.0 | 0.5884137119056182 |\n",
    "| alpha = 0.5 | / | 0.5894942298299759 | 0.5892178948780954 | 0.5893572173765593 | -1.0 | 0.5881458294354422 |\n",
    "| alpha = 0.6 | / | 0.5894942208412788 | 0.5891424955577386 | 0.5893220306242982 | -1.0 | 0.5874995443343022 |\n",
    "| alpha = 0.7 | / | 0.5894942118475708 | 0.5890600449542791 | 0.5892842034459883 | -1.0 | 0.5882114874616413 |\n",
    "| alpha = 0.8 | / | 0.5894942028488538 | 0.5889703921861977 | 0.5892437163792422 | -1.0 | 0.5873394456538039 |\n",
    "| alpha = 0.9 | / | 0.5894941938451262 | 0.5888733837169156 | 0.5892005498968458 | -1.0 | 0.5876962520402159 |\n",
    "| alpha = 1.0 | / | 0.589494184836389 | 0.588768863323186 | 0.5891546844077098 | -1.0 | 0.5865179031388511 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74156ddb",
   "metadata": {},
   "source": [
    "Question: \n",
    "\n",
    "1. what if the summary statics use entropy rather than simple summary?\n",
    "\n",
    "2. What if the original coding scheme is changed to sparse encoding? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9eaeae",
   "metadata": {},
   "source": [
    "**Using entropy as summary statistics**\n",
    "\n",
    "The following steps are divided into\n",
    "\n",
    "1) calculate the entropy stats as the summary stats\n",
    "\n",
    "2) call the main function to calculate the steps up to the summary stats \n",
    "\n",
    "3) Use least sqaure multiple regression to do the prediction\n",
    "\n",
    "4) Use correlation to evaluate whether the model is a good fit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "cb19d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyitlib import discrete_random_variable as drv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "0159d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_stats(all_pos,all_neg):\n",
    "    pos_sum = []\n",
    "    neg_sum = []\n",
    "    size = 268\n",
    "    for pos_mat in all_pos:\n",
    "        reshaped_pos = np.reshape(pos_mat, (268 * 268, 1))\n",
    "        droped_pos = reshaped_pos[reshaped_pos!=0.]\n",
    "        droped_pos = droped_pos[droped_pos!=-0.] #it doesn't matter whether it's zscored or not. \n",
    "        pos = ant.sample_entropy(droped_pos)\n",
    "        '''\n",
    "        Entropy candidate to consider: app_entropy, perm_entropy, sample_entropy\n",
    "        '''\n",
    "        pos_sum.append(pos)\n",
    "        \n",
    "    for neg_mat in all_neg:\n",
    "        reshaped_neg = np.reshape(neg_mat, (268 * 268, 1))\n",
    "        droped_neg = reshaped_neg[reshaped_neg!=0.]\n",
    "        droped_neg = droped_neg[droped_neg!=-0.]\n",
    "        neg = ant.sample_entropy(droped_neg)\n",
    "        neg_sum.append(neg)\n",
    "        \n",
    "    return pos_sum, neg_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "e4ccc17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_main():\n",
    "    size = (268,268) #set up the size of each matrix for each subject\n",
    "    filepath = '/Users/yutingzhang/Downloads/conmat_HCP_run_316_201216.mat'\n",
    "    print('finished with filepath')\n",
    "    array = read_file(filepath) #read in the file\n",
    "    pheno, sub_mat = extract_data(array) #extract the matrix and phenotype data\n",
    "    print('finished with a')\n",
    "    cor_mat, p_mat = matrix_gen(pheno,sub_mat) #extract the correlation matrix and p-value matrix for all subjects\n",
    "    pos_mat, neg_mat = edge_selection(cor_mat,p_mat,size) #edge selection with two matrices, one for pos, one for neg\n",
    "    print('finished with b')\n",
    "    sub_pos, sub_neg = element_multiply(sub_mat,pos_mat,neg_mat) #using the matrix to apply to the actual subject data to select that\n",
    "    sub_sum_pos,sub_sum_neg = entropy_stats(sub_pos,sub_neg) #calculate the summary statistics\n",
    "    print('finished with c')\n",
    "    return sub_sum_pos,sub_sum_neg,pheno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "3dd80402",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with filepath\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-d8281cbeaf95>:4: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(filepath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/scipy/stats/stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with b\n",
      "finished with c\n"
     ]
    }
   ],
   "source": [
    "sub_sum_pos,sub_sum_neg,pheno = entropy_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "80fdc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def OLS(sub_sum_pos,sub_sum_neg,pheno):\n",
    "    original = []\n",
    "    predict = []\n",
    "    \n",
    "    for i in range(np.shape(sub_sum_pos)[0]):\n",
    "        print(\"On subject %s\" %i)\n",
    "        loo_sub_sum_pos = np.delete(sub_sum_pos,[i])\n",
    "        single_sub_sum_pos = sub_sum_pos[i]\n",
    "        loo_sub_sum_neg = np.delete(sub_sum_neg,[i])\n",
    "        single_sub_sum_neg = sub_sum_neg[i]\n",
    "        loo_pheno = np.delete(pheno,[i])\n",
    "        single_sub_pheno = pheno[i]\n",
    "        X_train = np.vstack((np.reshape(loo_sub_sum_pos,(1,315)), np.reshape(loo_sub_sum_neg,(1,315))))\n",
    "        X_train = np.transpose(X_train)\n",
    "        Y_train = loo_pheno\n",
    "        reg = LinearRegression().fit(X_train, Y_train)\n",
    "        prediction = reg.predict(np.transpose(np.vstack((single_sub_sum_pos,single_sub_sum_neg))))\n",
    "        original.append(single_sub_pheno)\n",
    "        predict.append(prediction[0])\n",
    "    return original,predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "3fecc4ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On subject 0\n",
      "On subject 1\n",
      "On subject 2\n",
      "On subject 3\n",
      "On subject 4\n",
      "On subject 5\n",
      "On subject 6\n",
      "On subject 7\n",
      "On subject 8\n",
      "On subject 9\n",
      "On subject 10\n",
      "On subject 11\n",
      "On subject 12\n",
      "On subject 13\n",
      "On subject 14\n",
      "On subject 15\n",
      "On subject 16\n",
      "On subject 17\n",
      "On subject 18\n",
      "On subject 19\n",
      "On subject 20\n",
      "On subject 21\n",
      "On subject 22\n",
      "On subject 23\n",
      "On subject 24\n",
      "On subject 25\n",
      "On subject 26\n",
      "On subject 27\n",
      "On subject 28\n",
      "On subject 29\n",
      "On subject 30\n",
      "On subject 31\n",
      "On subject 32\n",
      "On subject 33\n",
      "On subject 34\n",
      "On subject 35\n",
      "On subject 36\n",
      "On subject 37\n",
      "On subject 38\n",
      "On subject 39\n",
      "On subject 40\n",
      "On subject 41\n",
      "On subject 42\n",
      "On subject 43\n",
      "On subject 44\n",
      "On subject 45\n",
      "On subject 46\n",
      "On subject 47\n",
      "On subject 48\n",
      "On subject 49\n",
      "On subject 50\n",
      "On subject 51\n",
      "On subject 52\n",
      "On subject 53\n",
      "On subject 54\n",
      "On subject 55\n",
      "On subject 56\n",
      "On subject 57\n",
      "On subject 58\n",
      "On subject 59\n",
      "On subject 60\n",
      "On subject 61\n",
      "On subject 62\n",
      "On subject 63\n",
      "On subject 64\n",
      "On subject 65\n",
      "On subject 66\n",
      "On subject 67\n",
      "On subject 68\n",
      "On subject 69\n",
      "On subject 70\n",
      "On subject 71\n",
      "On subject 72\n",
      "On subject 73\n",
      "On subject 74\n",
      "On subject 75\n",
      "On subject 76\n",
      "On subject 77\n",
      "On subject 78\n",
      "On subject 79\n",
      "On subject 80\n",
      "On subject 81\n",
      "On subject 82\n",
      "On subject 83\n",
      "On subject 84\n",
      "On subject 85\n",
      "On subject 86\n",
      "On subject 87\n",
      "On subject 88\n",
      "On subject 89\n",
      "On subject 90\n",
      "On subject 91\n",
      "On subject 92\n",
      "On subject 93\n",
      "On subject 94\n",
      "On subject 95\n",
      "On subject 96\n",
      "On subject 97\n",
      "On subject 98\n",
      "On subject 99\n",
      "On subject 100\n",
      "On subject 101\n",
      "On subject 102\n",
      "On subject 103\n",
      "On subject 104\n",
      "On subject 105\n",
      "On subject 106\n",
      "On subject 107\n",
      "On subject 108\n",
      "On subject 109\n",
      "On subject 110\n",
      "On subject 111\n",
      "On subject 112\n",
      "On subject 113\n",
      "On subject 114\n",
      "On subject 115\n",
      "On subject 116\n",
      "On subject 117\n",
      "On subject 118\n",
      "On subject 119\n",
      "On subject 120\n",
      "On subject 121\n",
      "On subject 122\n",
      "On subject 123\n",
      "On subject 124\n",
      "On subject 125\n",
      "On subject 126\n",
      "On subject 127\n",
      "On subject 128\n",
      "On subject 129\n",
      "On subject 130\n",
      "On subject 131\n",
      "On subject 132\n",
      "On subject 133\n",
      "On subject 134\n",
      "On subject 135\n",
      "On subject 136\n",
      "On subject 137\n",
      "On subject 138\n",
      "On subject 139\n",
      "On subject 140\n",
      "On subject 141\n",
      "On subject 142\n",
      "On subject 143\n",
      "On subject 144\n",
      "On subject 145\n",
      "On subject 146\n",
      "On subject 147\n",
      "On subject 148\n",
      "On subject 149\n",
      "On subject 150\n",
      "On subject 151\n",
      "On subject 152\n",
      "On subject 153\n",
      "On subject 154\n",
      "On subject 155\n",
      "On subject 156\n",
      "On subject 157\n",
      "On subject 158\n",
      "On subject 159\n",
      "On subject 160\n",
      "On subject 161\n",
      "On subject 162\n",
      "On subject 163\n",
      "On subject 164\n",
      "On subject 165\n",
      "On subject 166\n",
      "On subject 167\n",
      "On subject 168\n",
      "On subject 169\n",
      "On subject 170\n",
      "On subject 171\n",
      "On subject 172\n",
      "On subject 173\n",
      "On subject 174\n",
      "On subject 175\n",
      "On subject 176\n",
      "On subject 177\n",
      "On subject 178\n",
      "On subject 179\n",
      "On subject 180\n",
      "On subject 181\n",
      "On subject 182\n",
      "On subject 183\n",
      "On subject 184\n",
      "On subject 185\n",
      "On subject 186\n",
      "On subject 187\n",
      "On subject 188\n",
      "On subject 189\n",
      "On subject 190\n",
      "On subject 191\n",
      "On subject 192\n",
      "On subject 193\n",
      "On subject 194\n",
      "On subject 195\n",
      "On subject 196\n",
      "On subject 197\n",
      "On subject 198\n",
      "On subject 199\n",
      "On subject 200\n",
      "On subject 201\n",
      "On subject 202\n",
      "On subject 203\n",
      "On subject 204\n",
      "On subject 205\n",
      "On subject 206\n",
      "On subject 207\n",
      "On subject 208\n",
      "On subject 209\n",
      "On subject 210\n",
      "On subject 211\n",
      "On subject 212\n",
      "On subject 213\n",
      "On subject 214\n",
      "On subject 215\n",
      "On subject 216\n",
      "On subject 217\n",
      "On subject 218\n",
      "On subject 219\n",
      "On subject 220\n",
      "On subject 221\n",
      "On subject 222\n",
      "On subject 223\n",
      "On subject 224\n",
      "On subject 225\n",
      "On subject 226\n",
      "On subject 227\n",
      "On subject 228\n",
      "On subject 229\n",
      "On subject 230\n",
      "On subject 231\n",
      "On subject 232\n",
      "On subject 233\n",
      "On subject 234\n",
      "On subject 235\n",
      "On subject 236\n",
      "On subject 237\n",
      "On subject 238\n",
      "On subject 239\n",
      "On subject 240\n",
      "On subject 241\n",
      "On subject 242\n",
      "On subject 243\n",
      "On subject 244\n",
      "On subject 245\n",
      "On subject 246\n",
      "On subject 247\n",
      "On subject 248\n",
      "On subject 249\n",
      "On subject 250\n",
      "On subject 251\n",
      "On subject 252\n",
      "On subject 253\n",
      "On subject 254\n",
      "On subject 255\n",
      "On subject 256\n",
      "On subject 257\n",
      "On subject 258\n",
      "On subject 259\n",
      "On subject 260\n",
      "On subject 261\n",
      "On subject 262\n",
      "On subject 263\n",
      "On subject 264\n",
      "On subject 265\n",
      "On subject 266\n",
      "On subject 267\n",
      "On subject 268\n",
      "On subject 269\n",
      "On subject 270\n",
      "On subject 271\n",
      "On subject 272\n",
      "On subject 273\n",
      "On subject 274\n",
      "On subject 275\n",
      "On subject 276\n",
      "On subject 277\n",
      "On subject 278\n",
      "On subject 279\n",
      "On subject 280\n",
      "On subject 281\n",
      "On subject 282\n",
      "On subject 283\n",
      "On subject 284\n",
      "On subject 285\n",
      "On subject 286\n",
      "On subject 287\n",
      "On subject 288\n",
      "On subject 289\n",
      "On subject 290\n",
      "On subject 291\n",
      "On subject 292\n",
      "On subject 293\n",
      "On subject 294\n",
      "On subject 295\n",
      "On subject 296\n",
      "On subject 297\n",
      "On subject 298\n",
      "On subject 299\n",
      "On subject 300\n",
      "On subject 301\n",
      "On subject 302\n",
      "On subject 303\n",
      "On subject 304\n",
      "On subject 305\n",
      "On subject 306\n",
      "On subject 307\n",
      "On subject 308\n",
      "On subject 309\n",
      "On subject 310\n",
      "On subject 311\n",
      "On subject 312\n",
      "On subject 313\n",
      "On subject 314\n",
      "On subject 315\n",
      "Correlation of the predicted and original score is -0.06188538442233747.\n"
     ]
    }
   ],
   "source": [
    "original, predict = OLS(sub_sum_pos,sub_sum_neg,pheno)\n",
    "cor = np.corrcoef(np.asarray(original),np.asarray(predict))[0][1]\n",
    "print(\"Correlation of the predicted and original score is %s.\" %cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4294bfa",
   "metadata": {},
   "source": [
    "*Correlation of least sqaure model with different entropy as the summary stats*\n",
    "\n",
    "|  | app_entropy | perm_entropy | sample_entropy | \n",
    "| --- | --- | --- | --- | \n",
    "| cor | 0.01641536189675583 | 0.030878640150066024 | -0.06188538442233747 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca51f6a",
   "metadata": {},
   "source": [
    "*Ignore the following*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093dbc2c",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c244c09a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21., 20., 21., 21., 23., 19., 21., 20., 21., 21., 20., 21., 21.,\n",
       "       20., 23., 22., 21., 21., 20., 20., 20., 21., 21., 20., 20., 20.,\n",
       "       21., 21., 21., 21., 21., 22., 20., 21., 11., 21., 20., 11., 11.,\n",
       "       20., 21., 21., 21., 20., 11., 15., 20., 21., 21., 11., 21., 22.,\n",
       "       21., 21., 20., 21., 20., 23., 21., 20., 21., 20., 21., 20., 21.,\n",
       "       21., 21., 21., 21., 21., 15., 21., 21., 21., 22., 23., 20., 21.,\n",
       "       22., 20., 21., 21., 21., 11., 16., 20., 23., 21., 22., 20., 22.,\n",
       "       21., 21., 11., 23., 20., 20., 21., 21., 23.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply logistic regression\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr.fit(X_train,Y_train)\n",
    "logisticRegr.predict(X_test[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72b286b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1619047619047619"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the results for logistic regression\n",
    "logisticRegr.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e07fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply linear model, ordinary least squares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28888ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the data using pca_com\n",
    "pca_com.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0250667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "Principle_Component = pca.fit_transform(mat_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ccbd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data = Principle_Component, columns = ['Principal Component 1','Principal Component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32a699e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal Component 1</th>\n",
       "      <th>Principal Component 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.388122</td>\n",
       "      <td>2.989775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.251931</td>\n",
       "      <td>0.620193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.806732</td>\n",
       "      <td>4.823993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.364801</td>\n",
       "      <td>3.938786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363586</td>\n",
       "      <td>-1.682348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.650153</td>\n",
       "      <td>2.143270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>-1.102755</td>\n",
       "      <td>2.037828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>-0.426162</td>\n",
       "      <td>2.153671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.691928</td>\n",
       "      <td>5.480188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>-1.992476</td>\n",
       "      <td>3.080092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Principal Component 1  Principal Component 2\n",
       "0                -2.388122               2.989775\n",
       "1                 3.251931               0.620193\n",
       "2                -4.806732               4.823993\n",
       "3                -3.364801               3.938786\n",
       "4                 0.363586              -1.682348\n",
       "..                     ...                    ...\n",
       "311              -0.650153               2.143270\n",
       "312              -1.102755               2.037828\n",
       "313              -0.426162               2.153671\n",
       "314               0.691928               5.480188\n",
       "315              -1.992476               3.080092\n",
       "\n",
       "[316 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d0df864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5a3ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pickle file\n",
    "\n",
    "mat_rest = arrays['mat_rest']\n",
    "pickle_out = open(\"/Users/yutingzhang/Downloads/attention_array.pickle\",\"wb\")\n",
    "pickle.dump(mat_rest, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "sub_list = arrays['subList']\n",
    "pickle_out = open(\"/Users/yutingzhang/Downloads/attention_sublist.pickle\",\"wb\")\n",
    "pickle.dump(sub_list, pickle_out)\n",
    "pickle_out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34b64854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in pickle file as a list\n",
    "pickle_file = open('/Users/yutingzhang/Downloads/attention_array.pickle','rb')\n",
    "objects = []\n",
    "while True:\n",
    "    try:\n",
    "        objects.append(pickle.load(pickle_file))\n",
    "    except EOFError:\n",
    "        break\n",
    "pickle_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4915c272",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2bcf84188da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "pointer = arrays['id_name'][0][0]\n",
    "arrays['id_name'][pointer]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
